# GoldenSeed ‚Äî Deterministic High-Entropy Byte Streams

## Overview

> **‚ö†Ô∏è NOT FOR CRYPTOGRAPHY:** This library generates **deterministic** byte streams and **must not** be used for cryptographic purposes such as generating passwords, encryption keys, or any cryptographic key material. For cryptographic applications, use established cryptographically secure random number generators like Python's `secrets` module or `/dev/urandom`.

**GoldenSeed** is a library for generating **infinite, reproducible, high-entropy streams from tiny fixed seeds**. It provides deterministic pseudo-random number generation suitable for:

- üéÆ **Procedural Content Generation** - Games, simulations, and interactive demos
- üß™ **Reproducible Test Data** - Consistent test fixtures and fuzzing corpora
- üé® **Deterministic Noise Functions** - Perlin-like functions with exact cross-platform reproducibility
- ü§ù **Consensus Randomness** - Tie-breaking in distributed systems
- üíæ **Space-Efficient Storage** - Share large algorithmically generated datasets using tiny seeds
- üì¶ **Compression-Like Storage** - Store procedurally generated content as seeds

**Key Features:**
- ‚ôæÔ∏è **Infinite Streams** - Generate unlimited bytes from fixed seeds
- üîÑ **Reproducible** - Same seed always produces identical output
- üåê **Cross-Platform** - Consistent results across languages and architectures
- üì¶ **Tiny Seeds** - Store gigabytes of deterministic data in 16-32 bytes
- üöÄ **High Performance** - 10,000+ streams/second generation rate
- üéØ **Deterministic** - Perfect for procedural generation and testing

## License

This project is licensed under the GNU General Public License v3.0 or later (GPL-3.0-or-later). See the LICENSE file for details.

## Mathematical Foundation

The system uses deterministic mathematical operations for stream generation:

**Core Operations:**
- **Hash-based State Progression**: SHA-256 for deterministic state updates
- **Bit Shifting**: Left shift by 3 positions (equivalent to multiplication by 8)
- **XOR Folding**: Combining two halves of data to produce varied output
- **Basis Matching**: Bit comparison for selective bit extraction (~25-50% efficiency)

**Mathematical Constant Seeds:**
The system provides several mathematical constants as starting seeds:
- **Golden Ratio (œÜ)**: œÜ = (1 + ‚àö5)/2 ‚âà 1.618033988749895
- **Pi (œÄ)**: œÄ ‚âà 3.14159265358979323846
- **Euler's Number (e)**: e ‚âà 2.71828182845904523536
- **Square Root of 2 (‚àö2)**: ‚àö2 ‚âà 1.41421356237309504880

These provide reproducible, language-agnostic starting points for deterministic generation.

## Installation

### As a Python Package

Install the `golden-seed` package for programmatic access:

```bash
# Install from source (development mode)
pip install -e .

# Or install from PyPI (when published)
pip install golden-seed
```

## Important Usage Note

‚ö†Ô∏è **This library is NOT cryptographically secure and must NOT be used for:**
- Generating passwords or passphrases
- Creating encryption keys
- Generating cryptographic nonces or initialization vectors
- Any security-sensitive applications

‚úÖ **This library IS suitable for:**
- Procedural content generation in games
- Generating reproducible test data
- Creating deterministic noise for simulations
- Consensus randomness in distributed systems
- Space-efficient storage of procedural content

## Use Cases

### Procedural Content Generation

Generate infinite, reproducible game content from tiny seeds:

```python
from gq import UniversalQKD

# Create a world generator with a specific seed
world_gen = UniversalQKD()

# Generate terrain chunks - always the same for the same seed
for chunk_id in range(10):
    chunk_bytes = next(world_gen)
    # Use chunk_bytes to generate terrain, vegetation, etc.
    terrain_type = chunk_bytes[0] % 5  # 5 terrain types
    elevation = int.from_bytes(chunk_bytes[1:3], 'big') % 256
    print(f"Chunk {chunk_id}: terrain={terrain_type}, elevation={elevation}")
```

### Reproducible Testing

Create consistent test fixtures that are reproducible across environments:

```python
from gq import UniversalQKD

def generate_test_data(test_id):
    """Generate reproducible test data for a specific test"""
    generator = UniversalQKD()
    
    # Skip to test-specific position
    for _ in range(test_id * 100):
        next(generator)
    
    # Generate test data
    test_bytes = next(generator)
    return {
        'user_id': int.from_bytes(test_bytes[0:4], 'big'),
        'score': int.from_bytes(test_bytes[4:8], 'big') % 1000,
        'data': test_bytes.hex()
    }

# Same test always gets same data
test_data = generate_test_data(42)
print(f"Test 42 data: {test_data}")
```

### Deterministic Consensus

Tie-breaking in distributed systems without coordination:

```python
from gq import UniversalQKD

class ConsensusNode:
    def __init__(self):
        self.rng = UniversalQKD()
    
    def elect_leader(self, round_num, num_nodes):
        """All nodes elect the same leader deterministically"""
        # Skip to round-specific position
        for _ in range(round_num * 10):
            next(self.rng)
        
        leader_bytes = next(self.rng)
        leader_id = int.from_bytes(leader_bytes[0:4], 'big') % num_nodes
        return leader_id

# All nodes will elect the same leader
node1 = ConsensusNode()
node2 = ConsensusNode()
assert node1.elect_leader(5, 10) == node2.elect_leader(5, 10)
```

### Space-Efficient Storage

Store procedurally generated datasets as tiny seeds:

```python
from gq import UniversalQKD

# Instead of storing 1GB of procedural data,
# store just the seed and regenerate on demand
SEED_OFFSET = 12345  # This is all you need to store!

def regenerate_large_dataset(offset):
    """Regenerate gigabytes of data from a tiny seed"""
    generator = UniversalQKD()
    
    # Skip to specific offset
    for _ in range(offset):
        next(generator)
    
    # Generate large dataset on-demand
    dataset = []
    for i in range(1000):  # Generate 1000 entries
        data = next(generator)
        dataset.append(data)
    
    return dataset

# Regenerate the exact same dataset
dataset = regenerate_large_dataset(SEED_OFFSET)
print(f"Regenerated {len(dataset)} entries from seed offset {SEED_OFFSET}")
```


## Repository Structure

```
seed/
‚îú‚îÄ‚îÄ src/gq/                # Core library (importable package)
‚îÇ   ‚îú‚îÄ‚îÄ universal_qkd.py   # Universal stream generator
‚îÇ   ‚îú‚îÄ‚îÄ gqs1.py            # Test vector generation
‚îÇ   ‚îî‚îÄ‚îÄ cli/               # Command-line tools
‚îú‚îÄ‚îÄ formats/               # Seed format examples
‚îÇ   ‚îú‚îÄ‚îÄ golden_seed.hex    # Hex representation
‚îÇ   ‚îú‚îÄ‚îÄ golden_seed_16.bin # 16-byte binary
‚îÇ   ‚îî‚îÄ‚îÄ golden_seed_32.bin # 32-byte binary
‚îú‚îÄ‚îÄ tests/                 # Comprehensive test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_quantum_seed_foundations.py
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ examples/              # Example implementations
‚îî‚îÄ‚îÄ docs/                  # Documentation
```

### Standalone Scripts

The repository includes standalone CLI scripts:
- `src/gq/universal_qkd.py` - Universal stream generator
- `src/gq/gqs1.py` - Test vector generation
- `checksum/verify_binary_representation.py` - Binary verification with checksums

## Quick Start

### Python Package API

**Using the installed package (recommended):**

```python
from gq import UniversalQKD, GQS1

# Generate deterministic byte streams
generator = UniversalQKD()
stream = next(generator)
print(stream.hex())  # 3c732e0d04dac163a5cc2b15c7caf42c

# Generate test vectors
vectors = GQS1.generate_test_vectors(10)
print(vectors[0])  # a01611f01e8207a27c1529c3650c4838
```

**Importing from repository directories:**

```python
import sys
sys.path.insert(0, '/path/to/seed')

from src.gq.universal_qkd import universal_qkd_generator
from src.gq.gqs1 import generate_test_vectors

# Use the functions
generator = universal_qkd_generator()
stream = next(generator)
print(stream.hex())
```

### Command Line Tools

```bash
# After pip install -e .
gq-universal -n 10          # Generate 10 streams
gq-test-vectors -n 10       # Generate 10 test vectors
```

## Seed Values with Verified Checksums

The seed values are stored in the `formats/` directory with multiple representations for cross-platform compatibility.

### 16-byte seed (iœÜ):
```
0000000000000000A8F4979B77E3F93F
```
**File:** `formats/golden_seed_16.bin`

### 32-byte seed (iœÜ + 2√óœÜ):
```
0000000000000000A8F4979B77E3F93FA8F4979B77E3F93FA8F4979B77E3F93F
```
**File:** `formats/golden_seed_32.bin`

### Checksum Verification

All seed values include SHA256 checksum verification to ensure data integrity:

```bash
# Verify binary representation with checksums
python checksum/verify_binary_representation.py
```

## Usage in Any Language

1. **Read binary file as raw bytes**
   - Use `formats/golden_seed_16.bin` for 16-byte seed
   - Use `formats/golden_seed_32.bin` for 32-byte seed

2. **Interpret as IEEE 754 double-precision (little-endian complex)**
   - Bytes 0-7: Real part = 0.0
   - Bytes 8-15: Imaginary part = œÜ ‚âà 1.618033988749895

3. **Generate deterministic streams**
   - Use for procedural content generation
   - Deterministic tie-breaking in distributed systems
   - All nodes will generate identical streams

4. **No dependencies, no interpretation layer**
   - Pure binary representation
   - Works across all modern architectures
   - Language-agnostic implementation

## Example Code

> **Note:** The examples below are for demonstration. Remember: **NOT FOR CRYPTOGRAPHY**.

### Python

**Library Usage:**
```python
with open('formats/golden_seed_32.bin', 'rb') as f:
    seed = f.read(32)

# XOR with data for deterministic mixing
data = b'\x00' * 32  # Your data here
result = bytes(a ^ b for a, b in zip(data, seed))
```

**Universal Stream Generator:**

This repository includes a deterministic stream generator (`src/gq/universal_qkd.py`). This provides:

- Deterministic, synchronized generation across systems
- State progression via hashing
- Basis-matching (~50% efficiency)
- XOR folding for output variation
- Infinite stream generation
- **Cross-platform reproducibility**

```bash
# Generate 10 streams (default)
python src/gq/cli/universal.py

# Generate 100 streams
python src/gq/cli/universal.py -n 100

# Output in JSON format with binary representation
python src/gq/cli/universal.py -n 20 --json --binary

# Save to file
python src/gq/cli/universal.py -n 50 -o streams.txt

# Quiet mode (streams only, no headers)
python src/gq/cli/universal.py -n 5 --quiet
```

First stream (for cross-implementation validation):
```
3c732e0d04dac163a5cc2b15c7caf42c
```

**Test Vector Generation:**

For test vector generation and compliance testing:

```bash
# Generate 10 test vectors (default)
python src/gq/cli/gqs1.py

# Generate 100 test vectors
python src/gq/cli/gqs1.py -n 100

# Output in JSON format
python src/gq/cli/gqs1.py -n 20 --json
```

**Binary Representation Verification:**

Verify binary representations of seed values:

```bash
# Run verification for k=11 with seed_11=1234567891011
python checksum/verify_binary_representation.py
```

This tool demonstrates the relationship between seed values and their computed binary results using the formula:
```
result = (seed * 8) + k
```

Mathematical operations:
- Bit-shift left by 3: `seed << 3` (equivalent to `seed * 8`)
- Addition: Add offset parameter `k`
- XOR extraction: `result XOR (seed * 8)` to isolate the `k` contribution

Example output:
```
Seed_11 Bit Length: 41
Result Bit Length: 44
Binary Result (k=11): 0b10001111101110001111110110000100001000100011

Seed SHA256: 7f1665ab9f8c74fd60bd4fdcb10382b63727e10db9d568d385930695cc2f0454
Result SHA256: 677b205682ad566fcee652f80a4e8a538a265dc849da0d86fc0e5282b4cbf115
```

**Multi-Language Compiler:**

Generate implementations in any programming language:

```bash
# List all supported languages
python language_compiler.py --list

# Generate Python implementation
python language_compiler.py --language python -o my_generator.py

# Generate Rust implementation
python language_compiler.py --language rust -o generator.rs

# Generate ALL languages at once (Python, JS, TS, Rust, Go, C, Java)
python language_compiler.py --all
```

**Supported Languages:**
- **Python** - Full implementation with type hints
- **JavaScript** - ES6+ with BigInt support
- **TypeScript** - Strongly typed with interfaces
- **Rust** - Memory-safe with zero-cost abstractions
- **Go** - Concurrent-ready with big.Int
- **C** - High-performance native implementation
- **Java** - Enterprise-ready with BigInteger

Each generated implementation includes:
- Complete binary tap algorithm (bit-shift and XOR operations)
- Deterministic seed generation from digit sequences
- Example usage code
- Consistent API across languages

Example: Generate and use Rust implementation:
```bash
python language_compiler.py --language rust -o keygen.rs
rustc keygen.rs && ./keygen
```

### Language-Specific Examples

### C/C++
```c
#include <stdio.h>
#include <stdint.h>

int main() {
    FILE *f = fopen("formats/golden_seed_32.bin", "rb");
    if (!f) return 1;
    
    uint8_t seed[32];
    if (fread(seed, 1, 32, f) != 32) {
        fclose(f);
        return 1;
    }
    fclose(f);
    
    // XOR with block hash for tie-breaking
    uint8_t block_hash[32] = { /* your block hash */ };
    uint8_t result[32];
    for (int i = 0; i < 32; i++) {
        result[i] = block_hash[i] ^ seed[i];
    }
    return 0;
}
```

### Rust
```rust
use std::fs;

fn main() {
    let seed = fs::read("formats/golden_seed_32.bin").unwrap();
    let block_hash = vec![0u8; 32];  // Your block hash here
    let result: Vec<u8> = block_hash.iter()
        .zip(seed.iter())
        .map(|(a, b)| a ^ b)
        .collect();
}
```

### Go
```go
package main

import "os"

func main() {
    seed, _ := os.ReadFile("formats/golden_seed_32.bin")
    blockHash := make([]byte, 32)  // Your block hash here
    result := make([]byte, 32)
    for i := range seed {
        result[i] = blockHash[i] ^ seed[i]
    }
}
```

### JavaScript/Node.js
```javascript
const fs = require('fs');
const seed = fs.readFileSync('formats/golden_seed_32.bin');
const blockHash = Buffer.alloc(32);  // Your block hash here
const result = Buffer.from(blockHash.map((b, i) => b ^ seed[i]));
```

### Java
```java
import java.nio.file.Files;
import java.nio.file.Paths;

byte[] seed = Files.readAllBytes(Paths.get("formats/golden_seed_32.bin"));
byte[] blockHash = new byte[32];  // Your block hash here
byte[] result = new byte[32];
for (int i = 0; i < 32; i++) {
    result[i] = (byte)(blockHash[i] ^ seed[i]);
}
```

## Files

See the `formats/` directory for seed files:
- **formats/golden_seed.hex** - Hex representation
- **formats/golden_seed_16.bin** - 16-byte binary seed (iœÜ)
- **formats/golden_seed_32.bin** - 32-byte binary seed (iœÜ + 2√óœÜ)

See the `src/gq/` directory for core implementations:
- **src/gq/universal_qkd.py** - Universal stream generator
- **src/gq/gqs1.py** - Test vector generation

See the `checksum/` directory for verification tools:
- **checksum/verify_binary_representation.py** - Binary verification with checksums

See the `tests/` directory for comprehensive test suites:
- **tests/test_quantum_seed_foundations.py** - Stream validation (24 tests)
- **tests/test_binary_verification.py** - Binary Fusion Tap tests
- **tests/generate_quantum_test_vectors.py** - Generate 10,000+ test vectors
- **test_compression_capacity.py** - Data compression capacity testing (9 tests)

See the `docs/` directory for detailed documentation:
- **docs/ENTROPY_ANALYSIS.md** - Entropy analysis documentation
- **docs/COMPRESSION_TESTING.md** - Compression capacity testing and results

## Advanced Use Cases

This section demonstrates the flexibility and scalability of the repository through practical examples.

### Use Case 1: Deterministic Content Compression

Generate deterministic compression dictionaries for space-efficient data storage:

```python
from gq import universal_qkd_generator

def generate_compression_dictionary(seed_offset=0, dict_size=1000):
    """
    Generate a deterministic compression dictionary.
    
    The dictionary is reproducible from the seed, allowing both sender
    and receiver to use the same dictionary without transmission.
    """
    generator = universal_qkd_generator()
    
    # Skip to specific offset for different dictionaries
    for _ in range(seed_offset):
        next(generator)
    
    # Generate dictionary entries
    dictionary = {}
    for i in range(dict_size):
        key = next(generator)
        # Use key as dictionary entry
        dictionary[key.hex()[:8]] = key  # First 8 hex chars as lookup
    
    return dictionary

# Example: Generate compression dictionary
compression_dict = generate_compression_dictionary(seed_offset=0, dict_size=256)
print(f"Generated {len(compression_dict)} dictionary entries")

# Both sender and receiver can regenerate the same dictionary
# No need to transmit the dictionary itself
```

**Benefits:**
- Deterministic: Same dictionary on all nodes
- Space-efficient: No need to transmit dictionary
- Scalable: Generate dictionaries of any size
- Versioned: Use different offsets for different versions

### Use Case 2: Stream Validation and Versioning

Validate stream integrity and implement versioning:

```python
from gq import universal_qkd_generator
import hashlib

def validate_stream_sequence(start_stream, num_streams=10):
    """
    Validate a sequence of streams matches expected generation.
    
    Useful for verifying stream derivation in distributed systems.
    """
    generator = universal_qkd_generator()
    
    # Find starting position
    for position in range(100000):  # Search limit
        candidate = next(generator)
        if candidate == start_stream:
            print(f"Found start stream at position {position}")
            
            # Validate next streams
            valid_sequence = [candidate]
            for _ in range(num_streams - 1):
                valid_sequence.append(next(generator))
            
            return True, position, valid_sequence
    
    return False, -1, []

# Example: Validate stream sequence
expected_first_stream = bytes.fromhex("3c732e0d04dac163a5cc2b15c7caf42c")
is_valid, position, sequence = validate_stream_sequence(expected_first_stream, num_streams=5)

if is_valid:
    print(f"‚úì Valid stream sequence starting at position {position}")
    print(f"  Sequence: {[s.hex()[:16] + '...' for s in sequence]}")
else:
    print("‚úó Invalid stream sequence")

def implement_content_versioning(version_interval=1000):
    """
    Implement automatic content versioning.
    
    Content is versioned deterministically based on interval.
    """
    generator = universal_qkd_generator()
    
    current_version = 0
    version_streams = {}
    
    for stream_index in range(version_interval * 3):
        stream = next(generator)
        version = stream_index // version_interval
        
        if version != current_version:
            print(f"‚Üí Advancing to version {version}")
            current_version = version
        
        if version not in version_streams:
            version_streams[version] = []
        version_streams[version].append(stream)
    
    return version_streams

# Example: 3 versions with 1000 streams each
versions = implement_content_versioning(version_interval=1000)
print(f"\nGenerated {len(versions)} content versions")
for version_id, streams in versions.items():
    print(f"  Version {version_id}: {len(streams)} streams")
```

**Benefits:**
- Verifiable: Validate stream sequences
- Auditable: Trace stream derivation paths
- Automated: Implement versioning without manual intervention
- Deterministic: Same versioning schedule across all systems

### Use Case 3: Infinite Content Generation

Generate infinite streams of deterministic content for gaming, simulations, or testing:

```python
from gq import universal_qkd_generator
import struct

class InfiniteWorldGenerator:
    """
    Generate infinite deterministic world content.
    
    Uses key stream as entropy source for procedural generation.
    All nodes generate identical worlds from the same seed.
    """
    
    def __init__(self, seed_offset=0):
        self.generator = universal_qkd_generator()
        # Skip to specific world seed
        for _ in range(seed_offset):
            next(self.generator)
    
    def generate_chunk(self, chunk_x, chunk_z):
        """Generate a world chunk at coordinates (x, z)."""
        # Derive chunk-specific generator position
        chunk_seed = (chunk_x * 31) + (chunk_z * 17)
        
        # Generate deterministic content
        chunk_key = next(self.generator)
        
        # Convert key to chunk properties
        properties = {
            'biome': int.from_bytes(chunk_key[:2], 'big') % 10,
            'elevation': int.from_bytes(chunk_key[2:4], 'big') % 256,
            'vegetation': int.from_bytes(chunk_key[4:6], 'big') % 100,
            'temperature': int.from_bytes(chunk_key[6:8], 'big') % 100,
            'moisture': int.from_bytes(chunk_key[8:10], 'big') % 100,
        }
        
        return properties
    
    def generate_entity(self, entity_id):
        """Generate a unique entity with deterministic properties."""
        entity_key = next(self.generator)
        
        # Derive entity attributes from key
        entity = {
            'id': entity_id,
            'type': int.from_bytes(entity_key[:2], 'big') % 50,
            'health': int.from_bytes(entity_key[2:4], 'big') % 1000,
            'position_x': struct.unpack('d', entity_key[4:12])[0],
            'position_y': struct.unpack('d', entity_key[8:16])[0],
            'attributes': entity_key.hex()[:32],
        }
        
        return entity

# Example: Generate infinite world
world = InfiniteWorldGenerator(seed_offset=0)

print("Generating world chunks:")
for x in range(-2, 3):
    for z in range(-2, 3):
        chunk = world.generate_chunk(x, z)
        print(f"  Chunk ({x:+d}, {z:+d}): biome={chunk['biome']}, "
              f"elevation={chunk['elevation']}, vegetation={chunk['vegetation']}%")

print("\nGenerating entities:")
for entity_id in range(5):
    entity = world.generate_entity(entity_id)
    print(f"  Entity {entity_id}: type={entity['type']}, "
          f"health={entity['health']}, pos=({entity['position_x']:.2f}, {entity['position_y']:.2f})")

class InfiniteLevelGenerator:
    """Generate infinite game levels deterministically."""
    
    def __init__(self):
        self.generator = universal_qkd_generator()
    
    def generate_level(self, level_number):
        """Generate a complete game level."""
        # Skip to level-specific position
        for _ in range(level_number * 100):
            next(self.generator)
        
        # Generate level properties
        level_keys = [next(self.generator) for _ in range(10)]
        
        level = {
            'number': level_number,
            'difficulty': sum(level_keys[0]) % 10,
            'enemy_count': sum(level_keys[1]) % 50,
            'treasure_count': sum(level_keys[2]) % 20,
            'obstacles': [sum(k) % 100 for k in level_keys[3:8]],
            'boss_health': sum(level_keys[9]) * 10,
        }
        
        return level

# Example: Generate game levels
level_gen = InfiniteLevelGenerator()

print("\nGenerating game levels:")
for level_num in [1, 10, 100, 1000]:
    level = level_gen.generate_level(level_num)
    print(f"  Level {level_num}: difficulty={level['difficulty']}/10, "
          f"enemies={level['enemy_count']}, treasures={level['treasure_count']}")
```

**Benefits:**
- Infinite: Generate unlimited deterministic content
- Memory-efficient: Generate content on-demand, no storage needed
- Multiplayer-ready: All players see the same world
- Reproducible: Same seed = same world every time
- Scalable: Generate content at any scale without performance degradation

### Use Case 4: High-Performance Stream Generation

Optimize for maximum throughput in high-performance scenarios:

```python
from gq import universal_qkd_generator
import time

def benchmark_stream_generation(num_streams=100000):
    """Benchmark stream generation performance."""
    generator = universal_qkd_generator()
    
    start_time = time.time()
    streams = [next(generator) for _ in range(num_streams)]
    elapsed = time.time() - start_time
    
    streams_per_sec = num_streams / elapsed
    
    print(f"Performance Benchmark:")
    print(f"  Generated: {num_streams:,} streams")
    print(f"  Time: {elapsed:.2f} seconds")
    print(f"  Throughput: {streams_per_sec:,.0f} streams/sec")
    print(f"  Latency: {(elapsed / num_streams) * 1000:.3f} ms/stream")
    
    return streams

# Example: Benchmark performance
streams = benchmark_stream_generation(num_streams=100000)
print(f"  Memory: {len(streams) * 16 / 1024 / 1024:.2f} MB for {len(streams):,} streams")

def batch_stream_generation(batch_size=1000, num_batches=100):
    """Generate streams in batches for streaming applications."""
    generator = universal_qkd_generator()
    
    for batch_num in range(num_batches):
        batch = [next(generator) for _ in range(batch_size)]
        
        # Process batch (e.g., generate content, populate database)
        if batch_num % 10 == 0:
            print(f"  Processed batch {batch_num}: {len(batch)} streams")
        
        # Yield or process batch here
        yield batch

# Example: Batch processing
print("\nBatch stream generation:")
total_streams = 0
for batch in batch_stream_generation(batch_size=1000, num_batches=10):
    total_streams += len(batch)
print(f"Total streams generated: {total_streams:,}")
```

**Performance Characteristics:**
- **Throughput**: 10,000+ streams/second (depends on hardware)
- **Memory**: ~16 bytes per stream
- **Latency**: <0.1 ms per stream
- **Scalability**: Linear scaling with no degradation

### Use Case 5: Multi-Node Consensus

Implement deterministic consensus in distributed systems:

```python
from gq import universal_qkd_generator

class ConsensusProtocol:
    """
    Deterministic consensus using shared stream generation.
    
    All nodes generate identical streams, enabling leader election,
    tie-breaking, and randomized algorithms without coordination.
    """
    
    def __init__(self, node_id, total_nodes):
        self.node_id = node_id
        self.total_nodes = total_nodes
        self.generator = universal_qkd_generator()
    
    def elect_leader(self, round_number):
        """
        Elect a leader for the given round.
        
        All nodes compute the same leader deterministically.
        """
        # Skip to round-specific position
        for _ in range(round_number * 10):
            next(self.generator)
        
        # Generate leader selection stream
        leader_stream = next(self.generator)
        
        # Derive leader from stream
        leader_id = int.from_bytes(leader_stream[:4], 'big') % self.total_nodes
        
        return leader_id
    
    def resolve_tie(self, candidates, decision_id):
        """
        Resolve ties between candidates deterministically.
        
        All nodes pick the same winner without communication.
        """
        # Generate tie-breaker stream
        for _ in range(decision_id):
            next(self.generator)
        
        tie_breaker = next(self.generator)
        
        # Select winner based on stream
        winner_idx = int.from_bytes(tie_breaker[:4], 'big') % len(candidates)
        
        return candidates[winner_idx]
    
    def generate_random_beacon(self, epoch):
        """
        Generate a random beacon for the epoch.
        
        All nodes generate the same beacon value.
        """
        for _ in range(epoch * 100):
            next(self.generator)
        
        beacon = next(self.generator)
        
        return beacon.hex()

# Example: 5-node consensus network
nodes = [ConsensusProtocol(node_id=i, total_nodes=5) for i in range(5)]

print("Leader election across 10 rounds:")
for round_num in range(10):
    leaders = [node.elect_leader(round_num) for node in nodes]
    
    # All nodes should elect the same leader
    assert len(set(leaders)) == 1, "Nodes disagreed on leader!"
    
    print(f"  Round {round_num}: Node {leaders[0]} is leader")

print("\nTie resolution:")
candidates = ["Alice", "Bob", "Charlie", "Diana"]
for decision_id in range(5):
    winners = [node.resolve_tie(candidates, decision_id) for node in nodes]
    
    # All nodes should pick the same winner
    assert len(set(winners)) == 1, "Nodes disagreed on winner!"
    
    print(f"  Decision {decision_id}: {winners[0]} wins")

print("\nRandom beacon generation:")
for epoch in range(3):
    beacons = [node.generate_random_beacon(epoch) for node in nodes]
    
    # All nodes generate the same beacon
    assert len(set(beacons)) == 1, "Nodes disagreed on beacon!"
    
    print(f"  Epoch {epoch}: {beacons[0][:32]}...")
```

**Benefits:**
- **Coordination-free**: No network communication needed
- **Byzantine-resistant**: Deterministic, cannot be manipulated
- **Fair**: Provably unbiased
- **Efficient**: Instant computation, no consensus overhead

## Testing

Run the comprehensive test suite to validate all components:

```bash
# Install the package in development mode
pip install -e .

# Run all tests
python -m unittest discover -s . -p "test_*.py" -v

# Run specific test suites
python -m unittest test_quantum_seed_foundations -v    # Stream validation
python -m unittest test_binary_verification -v         # Binary Fusion Tap tests
python -m unittest test_compression_capacity -v        # Compression capacity tests

# Run tests from the tests/ directory
python -m unittest tests.test_quantum_seed_foundations -v

# Generate 10,000 test vectors for statistical analysis
python tests/generate_quantum_test_vectors.py 10000
```

### Compression Capacity Testing

The repository includes comprehensive **compression capacity testing** that evaluates how well the seed-based approach compresses deterministically generated data:

```bash
# Run compression capacity tests (9 tests)
python test_compression_capacity.py

# Or use unittest
python -m unittest test_compression_capacity.TestCompressionCapacity -v
```

**Test Coverage:**
- ‚úÖ Small data compression (1 KB) - **32x compression ratio**
- ‚úÖ Medium data compression (100 KB) - **3,200x compression ratio**
- ‚úÖ Large data compression (10 MB) - **327,680x compression ratio**
- ‚úÖ Data reproduction accuracy verification (100% accurate)
- ‚úÖ Compression efficiency scaling analysis
- ‚úÖ Entropy analysis and comparison
- ‚úÖ Decompression speed benchmarks

**Results Summary:**
- **Compression Ratios**: 32x to 327,680x depending on data size
- **Comparison**: Vastly outperforms GZIP, BZ2, and LZMA for deterministic data
- **Advantage**: Increases linearly with data size
- **Accuracy**: 100% data reproduction with SHA-256 verification

See **[docs/COMPRESSION_TESTING.md](docs/COMPRESSION_TESTING.md)** for detailed results, graphs, and analysis.

### Standardized Test Library (STL)

The repository includes a comprehensive **Standardized Test Library (STL)** with extensive testing coverage:

#### Edge Case Tests (`tests/test_edge_cases.py`)
Validates boundary conditions and edge cases:
- **22 tests** covering boundary values, invalid inputs, extreme parameters
- Tests all byte values (0-255) for basis matching
- Tests counter boundaries and overflow handling
- Tests XOR folding with various bit patterns
- Validates deterministic reproducibility across multiple runs

```bash
# Run edge case tests
python -m unittest tests.test_edge_cases -v
```

#### Scalability & Stress Tests (`tests/test_scalability_stress.py`)
Validates performance and scalability under high load:
- **20+ tests** covering large-scale generation and performance
- Tests generation of **10K, 100K, and 1M+ streams**
- Performance benchmarks: **10,000+ streams/second**
- Memory efficiency tests with continuous generation
- Tests no performance degradation over time
- Resource utilization under stress

```bash
# Run scalability tests
python -m unittest tests.test_scalability_stress -v

# Run specific performance benchmarks
python -m unittest tests.test_scalability_stress.TestPerformanceBenchmarks -v
```

**Performance Results:**
- ‚úÖ 10K streams in <1 second (~11,000 streams/sec)
- ‚úÖ 100K streams in <10 seconds
- ‚úÖ 1M streams in <2 minutes
- ‚úÖ No memory leaks detected
- ‚úÖ Consistent performance across batches

#### Multi-Seed Collision Tests (`tests/test_multi_seed_collision.py`)
Validates collision resistance and uniqueness:
- **18+ tests** covering collision detection and statistical properties
- Tests **100,000 unique streams** with no collisions
- Avalanche effect validation (50% bit difference on seed change)
- Bit distribution uniformity tests
- Hamming distance analysis
- Chi-square goodness of fit tests
- Statistical entropy validation

```bash
# Run collision tests
python -m unittest tests.test_multi_seed_collision -v

# Run specific collision test (generates 100K streams)
python -m unittest tests.test_multi_seed_collision.TestSeedCollisionResistance.test_no_collisions_within_single_seed_stream -v
```

**Collision Resistance Results:**
- ‚úÖ **100,000 consecutive streams** - no collisions detected
- ‚úÖ Average Hamming distance: ~64 bits (50% of 128 bits)
- ‚úÖ Uniform bit distribution (40-60% ones per position)
- ‚úÖ High entropy across stream (>7.0 bits/byte)

#### Cross-Platform Determinism Tests (`tests/test_cross_platform_determinism.py`)
Validates deterministic behavior across platforms:
- **21 tests** covering platform independence and reproducibility
- Tests first stream matches specification: `3c732e0d04dac163a5cc2b15c7caf42c`
- Tests IEEE 754 floating-point consistency
- Tests hash function determinism
- Tests byte order independence
- Validates reproducibility across Python versions
- System information logging for debugging

```bash
# Run cross-platform tests
python -m unittest tests.test_cross_platform_determinism -v
```

**Determinism Results:**
- ‚úÖ Identical streams across all platforms
- ‚úÖ Works on Linux, Windows, macOS
- ‚úÖ Consistent across Python 3.8-3.12+
- ‚úÖ Endianness-independent
- ‚úÖ 100% reproducible across runs

### Test Coverage Summary

**Total Test Count**: **110+ tests** across all suites

| Test Suite | Tests | Coverage |
|------------|-------|----------|
| Stream Foundations | 24 | Core mathematical principles |
| GQS-1 Protocol | 25 | Test vector generation |
| Universal Stream Gen | 29 | Stream generation protocol |
| **STL Edge Cases** | **22** | **Boundary conditions** |
| **STL Scalability** | **20+** | **Performance & stress** |
| **STL Collisions** | **18+** | **Uniqueness & entropy** |
| **STL Cross-Platform** | **21** | **Determinism & portability** |
| **Compression Capacity** | **9** | **Data compression testing** |

**Expected Results:**
- ‚úÖ All 24 stream foundation tests pass
- ‚úÖ All 81+ STL tests pass
- ‚úÖ All 9 compression capacity tests pass
- ‚úÖ 100% deterministic reproducibility
- ‚úÖ Cross-platform compatibility verified
- ‚úÖ Statistical randomness tests pass
- ‚úÖ No collisions in 100,000+ stream generation
- ‚úÖ Performance: 10,000+ streams/second
- ‚úÖ Compression ratios: 32x to 327,680x

## Important Disclaimer

‚ö†Ô∏è **NOT FOR CRYPTOGRAPHIC USE**: This library generates deterministic pseudo-random streams and must **NOT** be used for:
- Password generation
- Cryptographic key material
- Security-sensitive applications
- Encryption or authentication

For cryptographic purposes, use established CSPRNG libraries like Python's `secrets` module, `/dev/urandom`, or other cryptographically secure random number generators.

‚úÖ **Appropriate uses include:**
- Procedural content generation (games, simulations)
- Reproducible test data and fixtures
- Deterministic noise functions
- Consensus randomness in distributed systems
- Space-efficient storage of procedural content

## Security

See [SECURITY.md](SECURITY.md) for security policy and vulnerability reporting.

## License

This project is licensed under the GNU General Public License v3.0 or later (GPL-3.0-or-later). See the LICENSE file for details.
